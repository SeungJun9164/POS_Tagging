{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://colab.research.google.com/github/bentrevett/pytorch-pos-tagging/blob/master/1%20-%20BiLSTM%20for%20PoS%20Tagging.ipynb#scrollTo=6QHsOPobm93C\n",
    "\n",
    "\n",
    "https://github.com/bentrevett/pytorch-pos-tagging\n",
    "\n",
    "https://wikidocs.net/66747"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 시리즈에서는 PyTorch 및 TorchText를 사용하여 입력 시퀀스의 모든 요소에 대한 출력을 생성하는 모델을 구축 하여 입력 텍스트의 각 토큰에 대한 품사(Pos)를 출력한다. 이를 통해 객체명 인식 (NER)에 사용될 수 있다. 객체명 인식을 포털 사이트에 적용을 하게 되면 품사(Pos)가 무엇인지에 따라 더 정확한 결과를 도출해낼 수 있다.https://blog.est.ai/2020/10/ner/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 노트북에서는 UDPOS (Universal Dependencies English Web Treebank) 데이터 세트를 사용하여 품사(Pos) 태그를 예측하기 위해 다 계층 양방향 LSTM (BiLSTM)을 구현할 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministric = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 데이터 세트에는 실제로 UD (Universal Dependency)(보편적 의존성) 태그와 PTB (Penn Treebank)(펜트리뱅크https://codedragon.tistory.com/9553) 태그의 두 가지 다른 태그 세트가 있습니다. 우리는 UD 태그에서만 모델을 훈련시킬 것이지만 대신에 어떻게 사용될 수 있는지 보여주기 위해 PTB 태그를 로드 할 것입니다.\n",
    "\n",
    "UD_TAGS는 UD 태그 처리 방법을 처리한다. 나중에 구축 할 TEXT 어휘에는 알 수 없는 토큰, 즉 우리 어휘 내에없는 토큰이 포함된다. 그러나 가능한 태그의 유한 세트를 다루기 때문에 알 수 없는 태그는 없다.\n",
    "\n",
    "PTB_TAGS도 UD_TAGS와 비슷하지만 PTB 태그를 처리한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(lower=True)\n",
    "UD_TAGS = data.Field(unk_token = None)\n",
    "PTB_TAGS = data.Field(unk_token = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = (('text', TEXT), ('udtags', UD_TAGS), ('ptbtags', PTB_TAGS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = datasets.UDPOS.splits(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples : 12543\n",
      "Number of validation examples : 2002\n",
      "Number of testing examples : 2077\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples : {len(train_data)}')\n",
    "print(f'Number of validation examples : {len(valid_data)}')\n",
    "print(f'Number of testing examples : {len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['al', '-', 'zaman', ':', 'american', 'forces', 'killed', 'shaikh', 'abdullah', 'al', '-', 'ani', ',', 'the', 'preacher', 'at', 'the', 'mosque', 'in', 'the', 'town', 'of', 'qaim', ',', 'near', 'the', 'syrian', 'border', '.'], 'udtags': ['PROPN', 'PUNCT', 'PROPN', 'PUNCT', 'ADJ', 'NOUN', 'VERB', 'PROPN', 'PROPN', 'PROPN', 'PUNCT', 'PROPN', 'PUNCT', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'PROPN', 'PUNCT', 'ADP', 'DET', 'ADJ', 'NOUN', 'PUNCT'], 'ptbtags': ['NNP', 'HYPH', 'NNP', ':', 'JJ', 'NNS', 'VBD', 'NNP', 'NNP', 'NNP', 'HYPH', 'NNP', ',', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'NNP', ',', 'IN', 'DT', 'JJ', 'NN', '.']}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data.examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['al', '-', 'zaman', ':', 'american', 'forces', 'killed', 'shaikh', 'abdullah', 'al', '-', 'ani', ',', 'the', 'preacher', 'at', 'the', 'mosque', 'in', 'the', 'town', 'of', 'qaim', ',', 'near', 'the', 'syrian', 'border', '.']\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data.examples[0])['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PROPN', 'PUNCT', 'PROPN', 'PUNCT', 'ADJ', 'NOUN', 'VERB', 'PROPN', 'PROPN', 'PROPN', 'PUNCT', 'PROPN', 'PUNCT', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'PROPN', 'PUNCT', 'ADP', 'DET', 'ADJ', 'NOUN', 'PUNCT']\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data.examples[0])['udtags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NNP', 'HYPH', 'NNP', ':', 'JJ', 'NNS', 'VBD', 'NNP', 'NNP', 'NNP', 'HYPH', 'NNP', ',', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'DT', 'NN', 'IN', 'NNP', ',', 'IN', 'DT', 'JJ', 'NN', '.']\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data.examples[0])['ptbtags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_FREQ = 2\n",
    "TEXT.build_vocab(train_data, min_freq = MIN_FREQ, vectors = 'glove.6B.100d', unk_init=torch.Tensor.normal_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "UD_TAGS.build_vocab(train_data)\n",
    "PTB_TAGS.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in TEXT vocabulary: 8866\n",
      "Unique tokens in UD_TAG vocabulary: 18\n",
      "Unique tokens in PTB_TAG vocabulary: 51\n"
     ]
    }
   ],
   "source": [
    "print(f'Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}')\n",
    "print(f'Unique tokens in UD_TAG vocabulary: {len(UD_TAGS.vocab)}')\n",
    "print(f'Unique tokens in PTB_TAG vocabulary: {len(PTB_TAGS.vocab)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 9076), ('.', 8640), (',', 7021), ('to', 5137), ('and', 5002), ('a', 3782), ('of', 3622), ('i', 3379), ('in', 3112), ('is', 2239), ('you', 2156), ('that', 2036), ('it', 1850), ('for', 1842), ('-', 1426), ('have', 1359), ('\"', 1296), ('on', 1273), ('was', 1244), ('with', 1216)]\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.freqs.most_common(20)) # 가장 많이 쓰이는 text 상위 20개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', 'NOUN', 'PUNCT', 'VERB', 'PRON', 'ADP', 'DET', 'PROPN', 'ADJ', 'AUX', 'ADV', 'CCONJ', 'PART', 'NUM', 'SCONJ', 'X', 'INTJ', 'SYM']\n"
     ]
    }
   ],
   "source": [
    "print(UD_TAGS.vocab.itos) # itos : 숫자 -> text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', 'NN', 'IN', 'DT', 'NNP', 'PRP', 'JJ', 'RB', '.', 'VB', 'NNS', ',', 'CC', 'VBD', 'VBP', 'VBZ', 'CD', 'VBN', 'VBG', 'MD', 'TO', 'PRP$', '-RRB-', '-LRB-', 'WDT', 'WRB', ':', '``', \"''\", 'WP', 'RP', 'UH', 'POS', 'HYPH', 'JJR', 'NNPS', 'JJS', 'EX', 'NFP', 'GW', 'ADD', 'RBR', '$', 'PDT', 'RBS', 'SYM', 'LS', 'FW', 'AFX', 'WP$', 'XX']\n"
     ]
    }
   ],
   "source": [
    "print(PTB_TAGS.vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NOUN', 34781), ('PUNCT', 23679), ('VERB', 23081), ('PRON', 18577), ('ADP', 17638), ('DET', 16285), ('PROPN', 12946), ('ADJ', 12477), ('AUX', 12343), ('ADV', 10548), ('CCONJ', 6707), ('PART', 5567), ('NUM', 3999), ('SCONJ', 3843), ('X', 847), ('INTJ', 688), ('SYM', 599)]\n"
     ]
    }
   ],
   "source": [
    "print(UD_TAGS.vocab.freqs.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NN', 26915), ('IN', 20724), ('DT', 16817), ('NNP', 12449), ('PRP', 12193), ('JJ', 11591), ('RB', 10831), ('.', 10317), ('VB', 9476), ('NNS', 8438), (',', 8062), ('CC', 6706), ('VBD', 5402), ('VBP', 5374), ('VBZ', 4578), ('CD', 3998), ('VBN', 3967), ('VBG', 3330), ('MD', 3294), ('TO', 3286), ('PRP$', 3068), ('-RRB-', 1008), ('-LRB-', 973), ('WDT', 948), ('WRB', 869), (':', 866), ('``', 813), (\"''\", 785), ('WP', 760), ('RP', 755), ('UH', 689), ('POS', 684), ('HYPH', 664), ('JJR', 503), ('NNPS', 498), ('JJS', 383), ('EX', 359), ('NFP', 338), ('GW', 294), ('ADD', 292), ('RBR', 276), ('$', 258), ('PDT', 175), ('RBS', 169), ('SYM', 156), ('LS', 117), ('FW', 93), ('AFX', 48), ('WP$', 15), ('XX', 1)]\n"
     ]
    }
   ],
   "source": [
    "print(PTB_TAGS.vocab.freqs.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 품사(Pos)들이 얼마나 쓰였는지 %로 확인 하기 위한 함수\n",
    "def tag_percentage(tag_counts):\n",
    "    total_count = sum([count for tag, count in tag_counts])\n",
    "    tag_counts_percentages = [(tag, count, count/total_count) for tag, count in tag_counts]\n",
    "    return tag_counts_percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag\t\tCount\t\tPercentage\n",
      "\n",
      "NOUN\t\t34781\t\t17.0%\n",
      "PUNCT\t\t23679\t\t11.6%\n",
      "VERB\t\t23081\t\t11.3%\n",
      "PRON\t\t18577\t\t 9.1%\n",
      "ADP\t\t17638\t\t 8.6%\n",
      "DET\t\t16285\t\t 8.0%\n",
      "PROPN\t\t12946\t\t 6.3%\n",
      "ADJ\t\t12477\t\t 6.1%\n",
      "AUX\t\t12343\t\t 6.0%\n",
      "ADV\t\t10548\t\t 5.2%\n",
      "CCONJ\t\t6707\t\t 3.3%\n",
      "PART\t\t5567\t\t 2.7%\n",
      "NUM\t\t3999\t\t 2.0%\n",
      "SCONJ\t\t3843\t\t 1.9%\n",
      "X\t\t847\t\t 0.4%\n",
      "INTJ\t\t688\t\t 0.3%\n",
      "SYM\t\t599\t\t 0.3%\n"
     ]
    }
   ],
   "source": [
    "print('Tag\\t\\tCount\\t\\tPercentage\\n')\n",
    "\n",
    "for tag, count, percent in tag_percentage(UD_TAGS.vocab.freqs.most_common()):\n",
    "    print(f'{tag}\\t\\t{count}\\t\\t{percent*100:4.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag\t\tCount\t\tPercentage\n",
      "\n",
      "NN\t\t26915\t\t13.2%\n",
      "IN\t\t20724\t\t10.1%\n",
      "DT\t\t16817\t\t 8.2%\n",
      "NNP\t\t12449\t\t 6.1%\n",
      "PRP\t\t12193\t\t 6.0%\n",
      "JJ\t\t11591\t\t 5.7%\n",
      "RB\t\t10831\t\t 5.3%\n",
      ".\t\t10317\t\t 5.0%\n",
      "VB\t\t9476\t\t 4.6%\n",
      "NNS\t\t8438\t\t 4.1%\n",
      ",\t\t8062\t\t 3.9%\n",
      "CC\t\t6706\t\t 3.3%\n",
      "VBD\t\t5402\t\t 2.6%\n",
      "VBP\t\t5374\t\t 2.6%\n",
      "VBZ\t\t4578\t\t 2.2%\n",
      "CD\t\t3998\t\t 2.0%\n",
      "VBN\t\t3967\t\t 1.9%\n",
      "VBG\t\t3330\t\t 1.6%\n",
      "MD\t\t3294\t\t 1.6%\n",
      "TO\t\t3286\t\t 1.6%\n",
      "PRP$\t\t3068\t\t 1.5%\n",
      "-RRB-\t\t1008\t\t 0.5%\n",
      "-LRB-\t\t973\t\t 0.5%\n",
      "WDT\t\t948\t\t 0.5%\n",
      "WRB\t\t869\t\t 0.4%\n",
      ":\t\t866\t\t 0.4%\n",
      "``\t\t813\t\t 0.4%\n",
      "''\t\t785\t\t 0.4%\n",
      "WP\t\t760\t\t 0.4%\n",
      "RP\t\t755\t\t 0.4%\n",
      "UH\t\t689\t\t 0.3%\n",
      "POS\t\t684\t\t 0.3%\n",
      "HYPH\t\t664\t\t 0.3%\n",
      "JJR\t\t503\t\t 0.2%\n",
      "NNPS\t\t498\t\t 0.2%\n",
      "JJS\t\t383\t\t 0.2%\n",
      "EX\t\t359\t\t 0.2%\n",
      "NFP\t\t338\t\t 0.2%\n",
      "GW\t\t294\t\t 0.1%\n",
      "ADD\t\t292\t\t 0.1%\n",
      "RBR\t\t276\t\t 0.1%\n",
      "$\t\t258\t\t 0.1%\n",
      "PDT\t\t175\t\t 0.1%\n",
      "RBS\t\t169\t\t 0.1%\n",
      "SYM\t\t156\t\t 0.1%\n",
      "LS\t\t117\t\t 0.1%\n",
      "FW\t\t93\t\t 0.0%\n",
      "AFX\t\t48\t\t 0.0%\n",
      "WP$\t\t15\t\t 0.0%\n",
      "XX\t\t1\t\t 0.0%\n"
     ]
    }
   ],
   "source": [
    "print(\"Tag\\t\\tCount\\t\\tPercentage\\n\")\n",
    "\n",
    "for tag, count, percent in tag_percentage(PTB_TAGS.vocab.freqs.most_common()):\n",
    "    print(f\"{tag}\\t\\t{count}\\t\\t{percent*100:4.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# print(device)\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN의 향상버전인 Bi-LSTM을 이용하여 모델 생성\n",
    "class BiLSTMPOSTagger(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout, pad_idx): \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim, padding_idx = pad_idx)\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_dim, \n",
    "                            hidden_dim, \n",
    "                            num_layers = n_layers, \n",
    "                            bidirectional = bidirectional,\n",
    "                            dropout = dropout if n_layers > 1 else 0)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)  \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        \n",
    "        # print(text.shape) # [text lengths, batch_size]       \n",
    "        \n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        # print(embedded.shape) # [text lengths, batch_size, embedding_dim]      \n",
    "\n",
    "        #outputs holds the backward and forward hidden states in the final layer\n",
    "        #hidden and cell are the backward and forward hidden and cell states at the final time-step\n",
    "        \n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        # print(outputs.shape) # [text lengths, batch_size, hidden_dim * 2]\n",
    "        # print(hidden.shape) # [n_layers * 2, batch_size, hidden_dim]\n",
    "        # print(cell.shape) # [n_layers * 2, batch_size, hidden_dim]\n",
    "  \n",
    "        predictions = self.fc(self.dropout(outputs))\n",
    "        # print(predictions.shape) # [text lengths, batch_size, len(UD_TAG.vocab) == output_dim]\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 128\n",
    "OUTPUT_DIM = len(UD_TAGS.vocab)\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.25\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model = BiLSTMPOSTagger(INPUT_DIM, \n",
    "                        EMBEDDING_DIM, \n",
    "                        HIDDEN_DIM, \n",
    "                        OUTPUT_DIM, \n",
    "                        N_LAYERS, \n",
    "                        BIDIRECTIONAL, \n",
    "                        DROPOUT, \n",
    "                        PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLSTMPOSTagger(\n",
       "  (embedding): Embedding(8866, 100, padding_idx=1)\n",
       "  (lstm): LSTM(100, 128, num_layers=2, dropout=0.25, bidirectional=True)\n",
       "  (fc): Linear(in_features=256, out_features=18, bias=True)\n",
       "  (dropout): Dropout(p=0.25, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 가중치 초기화\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.normal_(param.data, mean = 0, std = 0.1)\n",
    "        \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 1,522,010 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8866, 100])\n"
     ]
    }
   ],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "print(pretrained_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1117, -0.4966,  0.1631,  ...,  1.2647, -0.2753, -0.1325],\n",
       "        [-0.8555, -0.7208,  1.3755,  ...,  0.0825, -1.1314,  0.3997],\n",
       "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
       "        ...,\n",
       "        [ 0.9261,  2.3049,  0.5502,  ..., -0.3492, -0.5298, -0.1577],\n",
       "        [-0.5972,  0.0471, -0.2406,  ..., -0.9446, -0.1126, -0.2260],\n",
       "        [-0.4809,  2.5629,  0.9530,  ...,  0.5278, -0.4588,  0.7294]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embedding.weight.data.copy_(pretrained_embeddings) # 임베딩 벡터값 copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1117, -0.4966,  0.1631,  ...,  1.2647, -0.2753, -0.1325],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
      "        ...,\n",
      "        [ 0.9261,  2.3049,  0.5502,  ..., -0.3492, -0.5298, -0.1577],\n",
      "        [-0.5972,  0.0471, -0.2406,  ..., -0.9446, -0.1126, -0.2260],\n",
      "        [-0.4809,  2.5629,  0.9530,  ...,  0.5278, -0.4588,  0.7294]])\n"
     ]
    }
   ],
   "source": [
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM) # 1번쨰 임베딩 벡터에 PAD_IDX(0)으로 초기화\n",
    "# model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM) # 0번째 임베딩 벡터에 UNK_IDX(1)으로 초기화\n",
    "\n",
    "print(model.embedding.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAG_PAD_IDX = UD_TAGS.vocab.stoi[UD_TAGS.pad_token] \n",
    "\n",
    "# 레이블 데이터의 패딩 토큰은 비용 함수의 연산에 포함시키지도 않도록 레이블 데이터의 패딩 토큰을 무시\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = model.to(device)\n",
    "# criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_accuracy(preds, y, tag_pad_idx):\n",
    "    \"\"\"\n",
    "    배치 당 정확도를 반환 즉, 8/10이 맞으면 8이 아닌 0.8이 반환\n",
    "    \"\"\"\n",
    "    max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n",
    "    non_pad_elements = (y != tag_pad_idx).nonzero()\n",
    "    correct = max_preds[non_pad_elements].squeeze(1).eq(y[non_pad_elements])\n",
    "    return correct.sum() / torch.FloatTensor([y[non_pad_elements].shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, tag_pad_idx):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        # iterator : 인코딩 된 text\n",
    "        text = batch.text\n",
    "        #print(text.shape) # [text lengths, batch_size]\n",
    "        tags = batch.udtags\n",
    "        #print(tags.shape) # [text lengths, batch_size]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "             \n",
    "        predictions = model(text)\n",
    "        # print(predictions.shape) # [sent len, batch size, output dim]\n",
    "        \n",
    "        predictions = predictions.view(-1, predictions.shape[-1])\n",
    "        tags = tags.view(-1)    \n",
    "        #predictions = [text lengths * batch size, output dim]\n",
    "        #tags = [text lengths * batch size]\n",
    "        \n",
    "        loss = criterion(predictions, tags)\n",
    "                \n",
    "        acc = categorical_accuracy(predictions, tags, tag_pad_idx)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion, tag_pad_idx):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            text = batch.text\n",
    "            tags = batch.udtags\n",
    "            \n",
    "            predictions = model(text)\n",
    "            \n",
    "            predictions = predictions.view(-1, predictions.shape[-1])\n",
    "            tags = tags.view(-1)\n",
    "            \n",
    "            loss = criterion(predictions, tags)\n",
    "            \n",
    "            acc = categorical_accuracy(predictions, tags, tag_pad_idx)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-33-71845e0bc7a1>:6: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  ..\\torch\\csrc\\utils\\python_arg_parser.cpp:882.)\n",
      "  non_pad_elements = (y != tag_pad_idx).nonzero()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 47s\n",
      "\tTrain Loss: 1.337 | Train Acc: 58.14%\n",
      "\t Val. Loss: 0.677 |  Val. Acc: 78.71%\n",
      "Epoch: 02 | Epoch Time: 0m 47s\n",
      "\tTrain Loss: 0.481 | Train Acc: 84.90%\n",
      "\t Val. Loss: 0.508 |  Val. Acc: 83.09%\n",
      "Epoch: 03 | Epoch Time: 0m 48s\n",
      "\tTrain Loss: 0.350 | Train Acc: 88.96%\n",
      "\t Val. Loss: 0.445 |  Val. Acc: 85.30%\n",
      "Epoch: 04 | Epoch Time: 0m 47s\n",
      "\tTrain Loss: 0.292 | Train Acc: 90.71%\n",
      "\t Val. Loss: 0.407 |  Val. Acc: 86.23%\n",
      "Epoch: 05 | Epoch Time: 0m 46s\n",
      "\tTrain Loss: 0.253 | Train Acc: 92.02%\n",
      "\t Val. Loss: 0.397 |  Val. Acc: 86.28%\n",
      "Epoch: 06 | Epoch Time: 0m 48s\n",
      "\tTrain Loss: 0.226 | Train Acc: 92.82%\n",
      "\t Val. Loss: 0.381 |  Val. Acc: 87.09%\n",
      "Epoch: 07 | Epoch Time: 0m 49s\n",
      "\tTrain Loss: 0.206 | Train Acc: 93.40%\n",
      "\t Val. Loss: 0.372 |  Val. Acc: 87.45%\n",
      "Epoch: 08 | Epoch Time: 0m 48s\n",
      "\tTrain Loss: 0.192 | Train Acc: 93.89%\n",
      "\t Val. Loss: 0.362 |  Val. Acc: 87.59%\n",
      "Epoch: 09 | Epoch Time: 0m 47s\n",
      "\tTrain Loss: 0.176 | Train Acc: 94.30%\n",
      "\t Val. Loss: 0.356 |  Val. Acc: 88.34%\n",
      "Epoch: 10 | Epoch Time: 0m 47s\n",
      "\tTrain Loss: 0.167 | Train Acc: 94.62%\n",
      "\t Val. Loss: 0.350 |  Val. Acc: 88.28%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion, TAG_PAD_IDX)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion, TAG_PAD_IDX)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.365 |  Test Acc: 88.30%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('tut1-model.pt'))\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion, TAG_PAD_IDX)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} |  Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_sentence(model, sentence, text_field, tag_field): # device,\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    if isinstance(sentence, str):\n",
    "        nlp = spacy.load('en')\n",
    "        tokens = [token.text for token in nlp(sentence)]\n",
    "    else:\n",
    "        tokens = [token for token in sentence]\n",
    "\n",
    "    if text_field.lower:\n",
    "        tokens = [t.lower() for t in tokens]\n",
    "        \n",
    "    numericalized_tokens = [text_field.vocab.stoi[t] for t in tokens]\n",
    "\n",
    "    unk_idx = text_field.vocab.stoi[text_field.unk_token]\n",
    "    \n",
    "    unks = [t for t, n in zip(tokens, numericalized_tokens) if n == unk_idx]\n",
    "    \n",
    "    token_tensor = torch.LongTensor(numericalized_tokens)\n",
    "    \n",
    "    token_tensor = token_tensor.unsqueeze(-1)  #.to(device)\n",
    "         \n",
    "    predictions = model(token_tensor)\n",
    "    \n",
    "    top_predictions = predictions.argmax(-1)\n",
    "    \n",
    "    predicted_tags = [tag_field.vocab.itos[t.item()] for t in top_predictions]\n",
    "    \n",
    "    return tokens, predicted_tags, unks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'this', 'killing', 'of', 'a', 'respected', 'cleric', 'will', 'be', 'causing', 'us', 'trouble', 'for', 'years', 'to', 'come', '.', ']']\n"
     ]
    }
   ],
   "source": [
    "example_index = 1\n",
    "\n",
    "sentence = vars(train_data.examples[example_index])['text']\n",
    "actual_tags = vars(train_data.examples[example_index])['udtags']\n",
    "\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['respected', 'cleric']\n"
     ]
    }
   ],
   "source": [
    "tokens, pred_tags, unks = tag_sentence(model, \n",
    "#                                        device, \n",
    "                                       sentence, \n",
    "                                       TEXT, \n",
    "                                       UD_TAGS)\n",
    "\n",
    "print(unks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred. Tag\tActual Tag\tCorrect?\tToken\n",
      "\n",
      "PUNCT\t\tPUNCT\t\t✔\t\t[\n",
      "DET\t\tDET\t\t✔\t\tthis\n",
      "VERB\t\tNOUN\t\t✘\t\tkilling\n",
      "ADP\t\tADP\t\t✔\t\tof\n",
      "DET\t\tDET\t\t✔\t\ta\n",
      "ADJ\t\tADJ\t\t✔\t\trespected\n",
      "NOUN\t\tNOUN\t\t✔\t\tcleric\n",
      "AUX\t\tAUX\t\t✔\t\twill\n",
      "AUX\t\tAUX\t\t✔\t\tbe\n",
      "VERB\t\tVERB\t\t✔\t\tcausing\n",
      "PRON\t\tPRON\t\t✔\t\tus\n",
      "NOUN\t\tNOUN\t\t✔\t\ttrouble\n",
      "ADP\t\tADP\t\t✔\t\tfor\n",
      "NOUN\t\tNOUN\t\t✔\t\tyears\n",
      "PART\t\tPART\t\t✔\t\tto\n",
      "VERB\t\tVERB\t\t✔\t\tcome\n",
      "PUNCT\t\tPUNCT\t\t✔\t\t.\n",
      "PUNCT\t\tPUNCT\t\t✔\t\t]\n"
     ]
    }
   ],
   "source": [
    "print(\"Pred. Tag\\tActual Tag\\tCorrect?\\tToken\\n\")\n",
    "\n",
    "for token, pred_tag, actual_tag in zip(tokens, pred_tags, actual_tags):\n",
    "    correct = '✔' if pred_tag == actual_tag else '✘'\n",
    "    print(f\"{pred_tag}\\t\\t{actual_tag}\\t\\t{correct}\\t\\t{token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "sentence = 'The Queen will deliver a speech about the conflict in North Korea at 1pm tomorrow.'\n",
    "\n",
    "tokens, tags, unks = tag_sentence(model, \n",
    "#                                   device, \n",
    "                                  sentence, \n",
    "                                  TEXT, \n",
    "                                  UD_TAGS)\n",
    "\n",
    "print(unks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred. Tag\tToken\n",
      "\n",
      "DET\t\tthe\n",
      "NOUN\t\tqueen\n",
      "AUX\t\twill\n",
      "VERB\t\tdeliver\n",
      "DET\t\ta\n",
      "NOUN\t\tspeech\n",
      "ADP\t\tabout\n",
      "DET\t\tthe\n",
      "NOUN\t\tconflict\n",
      "ADP\t\tin\n",
      "PROPN\t\tnorth\n",
      "PROPN\t\tkorea\n",
      "ADP\t\tat\n",
      "NUM\t\t1\n",
      "NOUN\t\tpm\n",
      "NOUN\t\ttomorrow\n",
      "PUNCT\t\t.\n"
     ]
    }
   ],
   "source": [
    "print(\"Pred. Tag\\tToken\\n\")\n",
    "\n",
    "for token, tag in zip(tokens, tags):\n",
    "    print(f\"{tag}\\t\\t{token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
